
import streamlit as st
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    mean_squared_error,
    r2_score
)

def evaluation_classification(model, X_test, y_test):
    y_pred = model.predict(X_test)
    
    # Rapport sous forme de DataFrame
    rapport = classification_report(y_test, y_pred, output_dict=True)
    df_rapport = pd.DataFrame(rapport).transpose().round(2)
    # Affichage stylis√©
    st.dataframe(df_rapport.style.background_gradient(cmap="Blues", axis=1).format(precision=2))
    
    cm = confusion_matrix(y_test, y_pred)
    labels = sorted(set(y_test))
    fig, ax = plt.subplots()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=labels, yticklabels=labels, ax=ax)
    ax.set_xlabel("Valeurs pr√©dites")
    ax.set_ylabel("Valeurs r√©elles")
    ax.set_title("Matrice de confusion")
    
    col1, col2, col3 = st.columns([2, 1, 1])  # colonne centrale plus large pour le graph
    with col1:
        st.pyplot(fig)

def evaluation_regression(model, X_test, y_test):
    y_pred = model.predict(X_test)

    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)

    st.markdown("### üìà R√©sultats de la r√©gression")
    st.markdown(f"- **RMSE** : `{rmse:.2f}`")
    st.markdown(f"- **R¬≤** : `{r2:.2f}`")

    fig, ax = plt.subplots()
    ax.scatter(y_test, y_pred, alpha=0.6)
    ax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
    ax.set_xlabel("Valeurs r√©elles")
    ax.set_ylabel("Pr√©dictions")
    ax.set_title("Valeurs r√©elles vs. Pr√©dictions")
    st.pyplot(fig)

def afficher_evaluation(model, X_test, y_test, type_tache):
    if type_tache == "classification":
        evaluation_classification(model, X_test, y_test)
    else:
        evaluation_regression(model, X_test, y_test)


def fonctionnement(type_tache):
    if type_tache == "classification":
        st.markdown("### üìã Rapport de Classification")
        with st.expander("‚ÑπÔ∏è Fonctionnement"):
            st.info("""
                Cette section affiche des m√©triques d'√©valuation du mod√®le. Ces m√©triques seront diff√©rentes en fonction du type de t√¢che (Classification ou R√©gression) effectu√©e.\n
                Nous sommes ici sur de la **Classification**.\n
                - Interpr√©tation des lignes :\n
                    * 0, 1, 2... repr√©sentent les **Classes Pr√©dictives**\n
                    * accuracy repr√©sente l‚Äô**Exactitude Globale** : proportion totale de bonnes pr√©dictions\n
                    * macro avg	repr√©sente la **Moyenne Non Pond√©r√©e** des m√©triques par classe\n
                    * weighted avg repr√©sente la **Moyenne Pond√©r√©e** par le support (nombre d‚Äôexemples de chaque classe)\n
                - Interpr√©tation des colonnes :\n
                    * precision repr√©sente le nombre de pr√©dictions correctes parmi les pr√©dictions positives pour cette classe,\n
                    * recall montre combien des vrais √©l√©ments de cette classe ont √©t√© bien retrouv√©s,\n
                    * f1-score repr√©sente la **Moyenne Harmonique** entre pr√©cision et rappel (√©quilibre entre les deux)\n
                    * le support affiche le nombre de cas r√©els pour chaque classe
            """)
    else:
        st.markdown("### üìã Rapport de R√©gression")
        with st.expander("‚ÑπÔ∏è Fonctionnement"):
            st.info("""
                Cette section affiche des m√©triques d'√©valuation du mod√®le. Ces m√©triques seront diff√©rentes en fonction du type de t√¢che (Classification ou R√©gression) effectu√©e.\n
                Nous sommes ici sur de la **R√©gression**.\n
                Interpr√©tation des m√©triques :
                    * RMSE (Root Mean Squared Error) : Racine de la moyenne des carr√©s des erreurs\n
                        Plus bas = meilleures pr√©dictions (en unit√©s de la variable cible)
                    * MAE (Mean Absolute Error) : Moyenne des erreurs absolues\n
                        Plus bas = erreurs faibles, moins sensibles aux outliers
                    * R¬≤ (Coefficient de d√©termination) : Proportion de variance expliqu√©e par le mod√®le\n
                        1.0 = parfait, 0 = pire qu'une moyenne constante, peut √™tre < 0 si catastrophique
                    """)